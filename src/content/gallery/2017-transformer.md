---
title: "Attention Is All You Need"
author: "Ashish Vaswani, Noam Shazeer, et al."
date: "2017-06-12"
milestone: "The Architecture of LLMs"
abstract: "Discarded recurrence and convolutions entirely in favor of self-attention mechanisms. It achieved unprecedented parallelization and performance, laying the architectural groundwork for GPT, BERT, and the entire generative AI revolution."
pdf_link: "/papers/2017-transformer.pdf"
cover: "../../assets/gallery/2017-transformer.png"
---
